# Inconsistency and Hallucination in AI

## Inconsistency
Inconsistency refers to situations where an AI model gives
different answers to the same or very similar questions.

### Causes
- Randomness in model generation
- Lack of clear context
- Model limitations

## Hallucination
Hallucination occurs when an AI model generates information
that sounds correct but is factually wrong or not based on data.

### Causes
- Insufficient or missing training data
- Overgeneralization by the model
- Lack of grounding in external knowledge

## My Understanding
AI models do not truly "understand" information.
They predict the next word based on patterns, which can
lead to inconsistent or hallucinated responses.

## Note
This is a conceptual topic. Mitigation techniques will be
explored later.
